---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  message = FALSE,
  warning = FALSE
)
```

# hyRelease

`hyRelease` generates release hydrofabric artifacts for use in NextGen. The types of data in each release is actively being discussed [here](https://github.com/mikejohnson51/hyRelease/issues/4).

For now, we are generating releases for HUC01 units and CAMELS basins. These are stored in a Lynker Technologies AWS bucket. If you have credentials to these, data can be accessed in a number of ways. 

A general approach (implemented in R, but general in nature) in the following way for an example CAMELS basin draining to NWIS gage `01435000`.

### Set up Credentials

In order to connect to S3, you need to authenticate. One way easiest method is to set environment variables in a working session like this:

```{r, eval = FALSE}
Sys.setenv("AWS_ACCESS_KEY_ID"     = "XXXXX",
           "AWS_SECRET_ACCESS_KEY" = "XXXXX",
           "AWS_DEFAULT_REGION"    = "us-east-2")
```

Or sourcing it from a private file:

```{r}
source("private/aws.R")
```

Or setting them to the root of your system.

However you authenticate, our resources are stored in `us-east-2` but your access key IDs and secrets are individual.

## Getting data!

For NGEN applications a number of file formats are needed including CSV, JSON, geoJSON, and gpkg. The first three of these are files that can be read directly, while a gpkg has some intereesting database capabilites.

### Read files

First, lets see what files are in our CAMEL "bucket". AWS does not have an explict folder structure so files are defined by a bucket, and can be reduced by the file prefix. So, lets first find the files in the CAMELS/gage_0143500 "directory".

```{r}
library(aws.s3)
library(data.table)
library(dplyr)

rbindlist(get_bucket(bucket = "formulations-dev", prefix = "CAMELS/gage_01435000"))$Key
```

Onec we know what files are avaialble, we can read any of them by selecting the correct driver:

#### CSV

The pattern for reading files is identical regardless of format, the correct reader simply needs to be defined:

```{r}
csv_data = s3read_using(fread, object = "s3://formulations-dev/CAMELS/gage_01435000/parameters/nwm.csv")
glimpse(csv_data)
```

#### JSON


```{r}
library(jsonlite)

json_data = s3read_using(read_json, object = "s3://formulations-dev/CAMELS/gage_01435000/parameters/waterbody-params.json", simplifyVector = TRUE)
glimpse(json_data[[1]])
```

### geoJSON
```{r}
library(sf)

## READ from geoJSON
cat_data = s3read_using(read_sf, object = "s3://formulations-dev/CAMELS/gage_01435000/spatial/catchment_data.geojson")
plot(cat_data)
```

### Geopackage

```{r}
## READ from GPKG
### find available layers
s3read_using(st_layers, object = "s3://formulations-dev/CAMELS/gage_01435000/spatial/hydrofabric.gpkg")

### call flowpaths layer
fp_data = s3read_using(st_read, object = "s3://formulations-dev/CAMELS/gage_01435000/spatial/hydrofabric.gpkg", "flowpaths")


### plot and add nexus layer
{
  plot(fp_data$geom)
  plot(s3read_using(st_read, object = "s3://formulations-dev/CAMELS/gage_01435000/spatial/hydrofabric.gpkg", "nexus"), add = TRUE, pch = 16, col = "red")
}
```


## Super cool!

Because gpkgs are SQLite databases we can pass queries to the resource to extract exactly the features we need. For example, if we want to pull the just the junction nexus locations, we can do so.


```{r}
junction_nex =  sf::st_read(
    dsn = "/vsis3/formulations-dev/hydrofabric/CONUS-hydrofabric/ngen-release/01a/2021-10-22/hydrofabric.gpkg",
    query = "SELECT * FROM nexus WHERE nexus_type == 'junction'")


plot(junction_nex$geom, pch = 16, cex = .1)
```

NOTE: The `/vsis3/` path prefix allows us to access the gpkg as a virtual dataset though GDALs [capabilites](https://gdal.org/user/virtual_file_systems.html)
